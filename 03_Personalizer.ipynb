{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup Personalizer Resource\n",
        "\n",
        "In the next step, we setup and tune an Azure Personalizer Resource to prioritize the recommendations for individual users based on envinonrmental context and user preferences.\n",
        "\n",
        "First, you must setup the Azure Personalizer Resource you will use by completing the following steps.\n",
        "\n",
        "\n",
        "**1.** Create an [Azure Personalizer Resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesPersonalizer). **NOTE:** If you have already have a Personalizer resource you want to reuse, make sure to clear the data in the Azure portal for the resource, following these [instructions](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-manage-model#clear-data-for-your-learning-loop).\n",
        "\n",
        "**NOTE:** The \"Free F0\" Pricing Tier will be sufficient for this example.\n",
        "\n",
        "**2.** Go to the **Configuration** page on your Personalizer resource in the Azure Portal and set the update model frequency setting to 15 seconds as shown [here](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-settings#configure-model-update-frequency-for-model-training) and a reward wait time of 10 minutes in your Personalizer Service as shown [here](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-settings#configure-rewards-for-the-feedback-loop).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Simulation Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When Personalizer is intialized, the suggestions are only successful between 20% to 30% of the time (indicated by the reward score of 1 meaning the suggestion matched the user's preference). After running many requests, the system improves.  This notebook simulates users interacting with a website to select groceries to purchase (10,000 orders are simulated) to prepare the Personalizer to make suggestions for our grocery customers. \n",
        "\n",
        "We will also show that we can run an offline evaluation based on the results of the simulations. This allows Personalizer to review the data and suggest a better learning policy. Then we will apply the new learning policy and run the notebook again with 4,000 requests and see if the loop performs better.\n",
        "\n",
        "This notebook is based off this [tutorial](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/tutorial-use-azure-notebook-generate-loop-data).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**How the simulation works**\n",
        "\n",
        "For setting up the simulation, we selected a set of recommendations from the \"Popular\" results of our Intelligent Recommendations service.  You can choose to use any of the recommendation dataset options, but limit the results to 50 or less items for use in Personalizer.    We have generated a sample set of foods and customer preferences for use in this example.\n",
        "\n",
        "In this notebook we will create a Personalizer loop _system_ which suggests a food based on the customers (eaters) and their preferences which are stored in a [customers dataset](data/eaters.json). Information about the foods available in a [foods dataset](data/foods.json).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A summary of the customer information is:\n",
        "\n",
        "|Customer Name|Weather|Flavor|\n",
        "|--|--|--|\n",
        "|Alice<br>Bob<br>Cathy<br>Dave|Sunny<br>Rainy<br>Snowy|Salty<br>Sweet<br>Plain| \n",
        "\n",
        "To help Personalizer make the correct food selection for each person, the system also knows details about the foods.\n",
        "\n",
        "|Temperature|Flavor Profile|Texture|Meal|\n",
        "|--|--|--|--|\n",
        "|Hot<br>Cold|Sweet<br>Savory|Crunchy<br>Soft<br>|Snack<br>Dessert|\n",
        "\n",
        "The purpose of the Personalizer loop is to find the best match between the customers and the foods they like as much of the time as possible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Rank and reward calls**\n",
        "\n",
        "For each of the few thousand calls to the Personalizer service makes in our loop, the Azure notebook sends the **Rank** request to the REST API:\n",
        "\n",
        "* A unique ID for the Rank/Request event\n",
        "* Context - A random choice of the user, weather, and flavor desired - simulating a user on a website or mobile device looking for a purchase suggestion.\n",
        "* Features - _All_ the food data - from which Personalizer makes a suggestion\n",
        "\n",
        "The system receives the rank of the food choices, then compares that prediction with the customer's known choice for the same weather and flavor preference. If the known choice is the same as the predicted choice, the **Reward** of 1 is sent back to Personalizer. Otherwise the reward is 0. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Include the python modules**\n",
        "\n",
        "Include the required python modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586587372
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import random \n",
        "import requests\n",
        "import time\n",
        "import uuid\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Set Personalizer resource key and name**\n",
        "\n",
        "From the Azure portal, find your key and endpoint on the **Overview** page of your Personalizer resource. Change the value of `Your_Personalizer_url` to your Personalizer's Endpoint. Change the value of `Your_Personalizer_ResourceKey` to your Personalizer key (Go to \"Manage Keys\" and copy on of the values). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1657586589444
        }
      },
      "outputs": [],
      "source": [
        "# Replace 'personalization_base_url' and 'resource_key' with your valid endpoint values.  You can view these from the Personalizer Resource Azure Portal Page.\n",
        "personalization_base_url = \"Your_Personalizer_url\"  \n",
        "resource_key = \"Your_Personalizer_ResourceKey\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function: Current Date Time\n",
        "\n",
        "This function is used to note the start and end times of the iterative function, `iterations`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586591383
        }
      },
      "outputs": [],
      "source": [
        "# Print out current datetime\n",
        "def currentDateTime():\n",
        "    currentDT = datetime.datetime.now()\n",
        "    print (str(currentDT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function: Model Updated\n",
        "\n",
        "The `get_last_updated` function prints out the last modified date and time that the model was updated. \n",
        "\n",
        "The function uses a GET REST API to [get model properties](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/GetModelProperties). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586593316
        }
      },
      "outputs": [],
      "source": [
        "# ititialize variable for model's last modified date\n",
        "modelLastModified = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586594376
        }
      },
      "outputs": [],
      "source": [
        "def get_last_updated(currentModifiedDate):\n",
        "    \n",
        "    print('-----checking model')\n",
        "    \n",
        "    # get model properties\n",
        "    response = requests.get(personalization_model_properties_url, headers = headers, params = None)\n",
        "    \n",
        "    print(response)\n",
        "    print(response.json())\n",
        "    \n",
        "    # get lastModifiedTime\n",
        "    lastModifiedTime = json.dumps(response.json()[\"lastModifiedTime\"])\n",
        "    \n",
        "    if (currentModifiedDate != lastModifiedTime):\n",
        "        currentModifiedDate = lastModifiedTime\n",
        "        print(f'-----model updated: {lastModifiedTime}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function: Service Settings\n",
        "\n",
        "This function validates and returns the state of the service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586597201
        }
      },
      "outputs": [],
      "source": [
        "def get_service_settings():\n",
        "    \n",
        "    print('-----checking service settings')\n",
        "    \n",
        "    # get learning policy\n",
        "    response = requests.get(personalization_model_policy_url, headers = headers, params = None)\n",
        "    \n",
        "    print(response)\n",
        "    print(response.json())\n",
        "    \n",
        "    # get service settings\n",
        "    response = requests.get(personalization_service_configuration_url, headers = headers, params = None)\n",
        "    \n",
        "    print(response)\n",
        "    print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct URLs & Load Data\n",
        "\n",
        "Construct URLs for REST calls and read JSON data files to setup the customers and foods we will use in the simulation.\n",
        "\n",
        "The steps are:\n",
        "* build the URLs used for all API calls\n",
        "* sets the security header using your Personalizer resource key \n",
        "* sets the random seed for the Rank event ID\n",
        "* reads in the JSON data files\n",
        "* calls `get_last_updated` method - learning policy has been removed in example output\n",
        "* calls `get_service_settings` method\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586601993
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# build URLs\n",
        "personalization_rank_url = personalization_base_url + \"personalizer/v1.0/rank\"\n",
        "personalization_reward_url = personalization_base_url + \"personalizer/v1.0/events/\" #add \"{eventId}/reward\"\n",
        "personalization_model_properties_url = personalization_base_url + \"personalizer/v1.0/model/properties\"\n",
        "personalization_model_policy_url = personalization_base_url + \"personalizer/v1.0/configurations/policy\"\n",
        "personalization_service_configuration_url = personalization_base_url + \"personalizer/v1.0/configurations/service\"\n",
        "headers = {'Ocp-Apim-Subscription-Key' : resource_key, 'Content-Type': 'application/json'}\n",
        "\n",
        "# load user context (our customers' preferences)\n",
        "users = \"data/eaters.json\"\n",
        "\n",
        "# action features (the food choices available)\n",
        "snacks = \"data/foods.json\"\n",
        "\n",
        "# empty JSON for Rank request\n",
        "requestpath = \"data/example-rankrequest.json\"\n",
        "\n",
        "# initialize random\n",
        "random.seed(time.time())\n",
        "\n",
        "userpref = None \n",
        "rankactionsjsonobj = None \n",
        "actionfeaturesobj = None\n",
        "\n",
        "with open(users) as handle:\n",
        "    userpref = json.loads(handle.read())\n",
        "\n",
        "with open(snacks) as handle:\n",
        "    actionfeaturesobj = json.loads(handle.read())\n",
        "    \n",
        "with open(requestpath) as handle:\n",
        "    rankactionsjsonobj = json.loads(handle.read())  \n",
        "    \n",
        "get_last_updated(modelLastModified)\n",
        "get_service_settings()\n",
        "\n",
        "print(f'Customer count {len(userpref)}')\n",
        "print(f'Food count {len(actionfeaturesobj)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Confirm first REST calls**\n",
        "\n",
        "This previous cell is the first cell that calls out to Personalizer. \n",
        "\n",
        "Make sure the REST status code in the output is `<Response [200]>`. If you get an error, such as 404, but you are sure your resource key and name are correct, reload the notebook.\n",
        "\n",
        "Verify that the output's `rewardWaitTime` and `modelExportFrequency` are both set to 15 seconds. \n",
        "\n",
        "Also, Make sure the count of customers and foods are both 4. If you get an error, check that you uploaded all 3 JSON files. \n",
        "    \n",
        "```console\n",
        "<Response [200]>\n",
        "{'rewardWaitTime': 'PT10M', 'defaultReward': 0.0, 'rewardAggregation': 'earliest', 'explorationPercentage': 0.2, 'modelExportFrequency': 'PT15S', 'logRetentionDays': 90, 'lastConfigurationEditDate': '2022-07-12T16:40:44', 'learningMode': 'Online'}\n",
        "Customer count 4\n",
        "Food count 4\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up a metric chart\n",
        "\n",
        "Later in this tutorial, the long running process of 10,000 requests is visible from the browser with an updating text box. It may be easier to see in a chart or as a total sum, when the long running process ends. To view this information, use the metrics provided with the resource. You can create the chart now that you have completed a request to the service, then refresh the chart periodically while the long running process is going.\n",
        "\n",
        "1. In the Azure portal, select your Personalizer resource.\n",
        "1. In the resource navigation, select **Metrics** underneath Monitoring. \n",
        "1. In the chart, select **Add metric**.\n",
        "1. The resource and metric namespace are already set. You only need to select the metric of **successful calls** and the aggregation of **sum**.\n",
        "1. Change the time filter to the last 4 hours.\n",
        "\n",
        "    You should see three successful calls in the chart. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function: Generate Unique Event ID\n",
        "\n",
        "This function generates a unique ID for each rank call. The ID is used to identify the rank and reward call information. This value could come from a business process such as a web view ID or transaction ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586610933
        }
      },
      "outputs": [],
      "source": [
        "def add_event_id(rankjsonobj):\n",
        "    eventid = uuid.uuid4().hex\n",
        "    rankjsonobj[\"eventId\"] = eventid\n",
        "    return eventid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Function: Randomly Choose Customer, Weather, Flavor\n",
        "\n",
        "This function selects a customer, the weather, and a flavor preference, then adds those items to the JSON object to send to the Rank request.\n",
        "\n",
        "**NOTE:** If you modify the number of users, preferences, what the preference are, update this function.\n",
        "\n",
        "A sample of the customers and their preferences is shown here - only some preferences are shown for brevity.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"Alice\": {\n",
        "    \"Sunny\": {\n",
        "      \"Plain\": \"ice cream\",\n",
        "      \"Salty\": \"sliced cheese\",\n",
        "      \"Sweet\": \"ice cream\"\n",
        "    }, ...\n",
        "  },\n",
        "    \"Bob\": {\n",
        "    \"Sunny\": {\n",
        "      \"Plain\": \"cocoa drinks\",\n",
        "      \"Salty\": \"sliced cheese\",\n",
        "      \"Sweet\": \"ice cream\"\n",
        "    }, ...\n",
        "  }\n",
        "  \"Cathy\": {\n",
        "    \"Sunny\": {\n",
        "      \"Plain\": \"popcorn\",\n",
        "      \"Salty\": \"ice cream\",\n",
        "      \"Sweet\": \"cocoa drinks\"\n",
        "    }..\n",
        "  }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586617272
        }
      },
      "outputs": [],
      "source": [
        "def add_random_user_and_contextfeatures(namesoption, flavoroption, weatheroption, rankjsonobj):   \n",
        "    name = namesoption[random.randint(0,3)]  # we have 4 users to select from randomly\n",
        "    flavor = flavoroption[random.randint(0,2)]  # we have 3 flavor choices\n",
        "    weather = weatheroption[random.randint(0,2)]  # we have 3 weather options\n",
        "    rankjsonobj['contextFeatures'] = [{'weather': weather, 'flavor': flavor, 'name': name}]\n",
        "    return [name, flavor, weather]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Function: Add Foods to Rank Request\n",
        "\n",
        "This function adds the list of foods to the JSON object (`rankjsonobj`) to send to the Rank request. \n",
        "\n",
        "An example of a single food item's features is: \n",
        "\n",
        "```json\n",
        "\n",
        "    {\n",
        "      \"id\": \"popcorn\",\n",
        "      \"features\": [\n",
        "        {\n",
        "          \"type\": \"hot\",\n",
        "          \"meal\": \"snack\",\n",
        "          \"texture\": \"crunchy\",\n",
        "          \"flavor\": \"savory\"\n",
        "            \n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586626237
        }
      },
      "outputs": [],
      "source": [
        "def add_action_features(rankjsonobj):\n",
        "    rankjsonobj[\"actions\"] = actionfeaturesobj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Function: Get Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function is called after the Rank API is called, for each iteration.  It will compare the Rank API's prediction with known customer preferences (based on weather and flavor choice) to determine what Reward should be given (1 if it matches, 0 if it doesn't)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586627936
        }
      },
      "outputs": [],
      "source": [
        "def get_reward_from_simulated_data(name, weather, flavor, prediction):\n",
        "    if(userpref[name][weather][flavor] == str(prediction)):\n",
        "        return 1 \n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function: Iterations\n",
        "\n",
        "The Iterations function runs the simulation by looping through calls to Rank and Reward as many times as configured (10,000 in this example).\n",
        "\n",
        "The Iterations function does the following:\n",
        "\n",
        "1.  Select a random Customer \n",
        "2.  Get the food list\n",
        "3.  Send the customer and food list to the Rank API. \n",
        "4.  Compares the prediction returned by the Rank API with the customer's known preferences and determines the reward (1 if it matches, 0 if not) \n",
        "5.  Send the reward score back to the Personalizer service. \n",
        "\n",
        "The loop runs for `num_requests` times - Personalizer needs a few thousand calls to Rank and Reward to create a model. \n",
        "\n",
        "**NOTE:** If changing the preference names, or adding/removing any, this function should be modified.\n",
        "\n",
        "An example of the JSON sent to the Rank API follows. The list of foods (actions) is not complete, for brevity, but all are sent. You can see the entire JSON for foods in `foods.json`.\n",
        "\n",
        "```json\n",
        "{\n",
        "\t'contextFeatures': [\n",
        "\t\t{\n",
        "\t\t\t'weather': 'Rainy', \n",
        "\t\t\t'flavor': 'Salty', \n",
        "\t\t\t'name': 'Dave'\n",
        "\t\t}\n",
        "\t], \n",
        "\t'actions': [\n",
        "\t\t{\n",
        "\t\t'id': 'popcorn', \n",
        "\t\t'features': [{'type': 'hot', 'meal': 'snack', 'texture': 'crunchy', 'flavor': 'savory'}]\n",
        "\t\t},\n",
        "\n",
        "\t\trest of the foods list are the actions to select from...\n",
        "\t\n",
        "\t], \n",
        "\t'excludedActions': [], \n",
        "\t'eventId': '3c86b4ba96204a66b811876bfb6439b4', 'deferActivation': False\n",
        "}\n",
        "```\n",
        "\n",
        "JSON response from the Rank API, indicating all 4 actions have equal probability.  In this case, it will select popcorn.\n",
        "\n",
        "```\n",
        "{\n",
        "\t'ranking': [\n",
        "\t\t{'id': 'popcorn', 'probability': 0.25}, \n",
        "\t\t{'id': 'sliced cheese', 'probability': 0.25}, \n",
        "\t\t{'id': 'cocoa drinks', 'probability': 0.25}, \n",
        "\t\t{'id': 'ice cream', 'probability': 0.25}\n",
        "\t], \n",
        "\t'eventId': '3c86b4ba96204a66b811876bfb6439b4', \n",
        "\t'rewardActionId': 'popcorn'\n",
        "}\n",
        "```\n",
        "\n",
        "Finally, each loop shows the random selection of user, weather, flavor choice, and what reward is determined. The reward of 1 indicates the Personalizer resource selected a food for the given user, weather, and flavor that matched their preferences.\n",
        "\n",
        "In this example, Dave's preferences match the selection of popcorn, so the Reward is 1.\n",
        "\n",
        "```console\n",
        "Customer: Dave\n",
        "Flavor Preference: Salty \n",
        "Weather: Rainy\n",
        "Suggestion: popcorn \n",
        "Reward: 1\n",
        "```\n",
        "\n",
        "The function uses:\n",
        "\n",
        "* Rank: a POST REST API to [get rank](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Rank). \n",
        "* Reward: a POST REST API to [report reward](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Reward)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657586629934
        }
      },
      "outputs": [],
      "source": [
        "def iterations(n, modelCheck, jsonFormat):\n",
        "\n",
        "    i = 1\n",
        "    \n",
        "    # default reward value - assumes failed prediction\n",
        "    reward = 0\n",
        "\n",
        "    # Print out dateTime\n",
        "    #currentDateTime()\n",
        "\n",
        "    # collect results to aggregate in graph\n",
        "    total = 0\n",
        "    rewards = []\n",
        "    count = []\n",
        "\n",
        "    # default list of user, weather, time of day\n",
        "    namesopt = ['Dave','Alice', 'Bob', 'Cathy']\n",
        "    flavoropt = ['Salty', 'Sweet', 'Plain']\n",
        "    weatheropt = ['Sunny', 'Rainy', 'Snowy']\n",
        "\n",
        "   \n",
        "    while(i <= n):\n",
        "\n",
        "        # create unique id to associate with an event\n",
        "        eventid = add_event_id(jsonFormat)\n",
        "\n",
        "        # generate a random sample\n",
        "        [name, flavor, weather] = add_random_user_and_contextfeatures(namesopt, flavoropt, weatheropt, jsonFormat)\n",
        "\n",
        "        # add action features to rank\n",
        "        add_action_features(jsonFormat) \n",
        "\n",
        "        # show JSON to send to Rank\n",
        "        #print('To: ', jsonFormat)    \n",
        "\n",
        "        # choose an action - get prediction from Personalizer\n",
        "        response = requests.post(personalization_rank_url, headers = headers, params = None, json = jsonFormat)\n",
        "\n",
        "        # show Rank prediction \n",
        "        #print ('From: ',response.json())    \n",
        "\n",
        "        # compare personalization service recommendation with the simulated data to generate a reward value\n",
        "        prediction = json.dumps(response.json()[\"rewardActionId\"]).replace('\"','')\n",
        "        reward = get_reward_from_simulated_data(name, weather, flavor, prediction)\n",
        "\n",
        "        # show result for iteration\n",
        "        print(f'   {i} {currentDateTime()} {name} {weather} {flavor} {prediction} {reward} ')\n",
        "\n",
        "        # send the reward to the service \n",
        "        response = requests.post(personalization_reward_url + eventid + \"/reward\", headers = headers, params= None, json = { \"value\" : reward })\n",
        "\n",
        "        # for every N rank requests, compute total correct  \n",
        "        total = total + reward\n",
        "\n",
        "        # every N iteration, get last updated model date and time\n",
        "        if(i % modelCheck == 0):\n",
        "\n",
        "            print(\"**** 10% of loop found\")\n",
        "            get_last_updated(modelLastModified) \n",
        "\n",
        "        # aggregate so chart is easier to read\n",
        "        if(i % 100 == 0):\n",
        "            print(\"**** aggregating rewards\")\n",
        "            rewards.append(total)\n",
        "            count.append(i)\n",
        "            total = 0\n",
        "\n",
        "        i = i + 1\n",
        "        \n",
        "    # Print out dateTime\n",
        "    #currentDateTime()\n",
        "    \n",
        "    return [count, rewards]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run for 10,000 iterations\n",
        "\n",
        "Run the Personalizer loop for 10,000 iterations. This is a long running event - at least an hour, can be more depending on your resources. Do not close the browser running the notebook, you will continue to see results scroll while it runs. Refresh the metrics chart in the Azure portal periodically to see the total calls to the service. When you have around 20,000 calls, a rank and reward call for each iteration of the loop, the iterations are done. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657589410829
        }
      },
      "outputs": [],
      "source": [
        "# max iterations\n",
        "num_requests = 10000\n",
        "\n",
        "# check last mod date N% of time - currently 10%\n",
        "lastModCheck = int(num_requests * .10)\n",
        "\n",
        "jsonTemplate = rankactionsjsonobj\n",
        "\n",
        "# main iterations\n",
        "[count, rewards] = iterations(num_requests, lastModCheck, jsonTemplate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function: Chart Results\n",
        "\n",
        "Create a chart from the `count` and `rewards` tracked in the simulation to see if there was improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657591677224
        }
      },
      "outputs": [],
      "source": [
        "def createChart(x, y):\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel(\"Batch of rank events\")\n",
        "    plt.ylabel(\"Correct recommendations per batch\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review Results of Simulation\n",
        "\n",
        "Run the `createChart` function with the accumlated values from the iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1657591679898
        }
      },
      "outputs": [],
      "source": [
        "createChart(count, rewards) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This chart shows the success of the model for the current default learning policy. \n",
        "\n",
        "The ideal target that by the end of the test, the loop is averaging a success rate that is close to one hundred percent minus the exploration. The default setting of exploration is 20%. \n",
        "\n",
        "`100-20=80`\n",
        "\n",
        "This exploration setting is found in the Azure portal, for the Personalizer resource, in the Configurations tab, under the RESOURCE MANAGEMENT section "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Offline Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run an offline evaluation\n",
        "\n",
        "In order to find a better learning policy, based on your data to the Rank API, run an [offline evaluation](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/concepts-offline-evaluation) in the portal for your Personalizer loop.\n",
        "\n",
        "1. In the Azure portal, open the Personalizer resource's **Evaluations** page.\n",
        "1. Select **Offline Evaluations**, then **+ Create Evaluation**.\n",
        "1. Enter the required data of evaluation name, and date range for the loop evaluation. The date range should include the days you are focusing on for your evaluation (in our case, the current day(s) you ran the 10,000 iterations). \n",
        "\n",
        "    The purpose of running this offline evaluation is to determine if there is a better learning policy for the features and actions used in this loop. To find that better learning policy, make sure **Optimization discovery** is turned on.\n",
        "\n",
        "1. Select **OK** to begin the evaluation. \n",
        "1. This **Evaluations** page lists the new evaluation and its current status. Depending on how much data you have, this evaluation can take some time. You can come back to this page after a few minutes to see the results. \n",
        "1. When the evaluation is completed, select the evaluation then select **Compare the score of your application with other potential learning settings**. This shows the available learning policies and how they would behave with the data. \n",
        "1. Select the top-most learning policy in the table and select **Apply**. This applies the _best_ learning policy to your model and retrains. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure for new Learning Policy\n",
        "\n",
        "\n",
        "Before running again, change the update model frequency to 5 minutes.\n",
        "\n",
        "1. In the Azure portal, still on the Personalizer resource, selec the Configurations tab, under the Resource Management section. \n",
        "1. Change the **model update frequency**  to 5 minutes and select **Save**.\n",
        "\n",
        "Learn more about the [reward wait time](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/concept-rewards#reward-wait-time) and [model update frequency](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-settings#model-update-frequency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Then verify the new learning policy and times are set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_service_settings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify that the output's `modelExportFrequency` are both set to 5 minutes. \n",
        "    \n",
        "```console\n",
        "-----checking service settings\n",
        "<Response [200]>\n",
        "{'name': '777e726360cd40a1af32d2e825400975', 'arguments': '--cb_explore_adf --epsilon 0.20000000298023224 --dsjson --cb_type mtr --marginal i -q Fi -q Fj -l 0.1 --power_t 0'}\n",
        "<Response [200]>\n",
        "{'rewardWaitTime': 'PT10M', 'defaultReward': 0.0, 'rewardAggregation': 'earliest', 'explorationPercentage': 0.2, 'modelExportFrequency': 'PT5M', 'logRetentionDays': 90, 'lastConfigurationEditDate': '2022-07-12T18:26:32', 'learningMode': 'Online'}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run 4,000 iterations with new Learning Policy\n",
        "\n",
        "Run the loop again, but for only 4,000 iterations. Refresh the metrics chart in the Azure portal periodically to see the total calls to the service.  This is a long running event. Do not close the browser running the notebook. Refresh the metrics chart in the Azure portal periodically to see the total calls to the service. When you have around 8,000 calls showing in the chart, a rank and a reward call for each iteration of the loop, the iterations are done. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max iterations\n",
        "num_requests = 4000\n",
        "\n",
        "# check last mod date N% of time - currently 10%\n",
        "lastModCheck2 = int(num_requests * .10)\n",
        "\n",
        "jsonTemplate2 = rankactionsjsonobj\n",
        "\n",
        "# main iterations\n",
        "[count2, rewards2] = iterations(num_requests, lastModCheck2, jsonTemplate2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "createChart(count2,rewards2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second chart should show a visible increase in Rank predictions aligning with user preferences fairly early in the iterations run, proving it's a better learning policy for us to use with our grocery recommender, at least for now!  You will want to continue to monitor the performance over time as more users interact with the system to determine if changes should be made."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
